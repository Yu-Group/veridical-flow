{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "correct-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from vflow import Vset, build_vset, init_args, dict_to_df, perturbation_stats\n",
    "from vflow.pipeline import build_graph\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "\n",
    "pd.options.display.max_rows = 8\n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde466a3",
   "metadata": {},
   "source": [
    "# Feature Importance Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3e373",
   "metadata": {},
   "source": [
    "In this example, we will probe the the stability of the permutation feature importance metric for random forest relative to data resampling, data preprocessing, and model hyperparameter perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd40726",
   "metadata": {},
   "source": [
    "`vflow` supports automatic parallelization using `ray`. We can use `ray` to compute downstream results by setting `is_async=True` when constructing a `Vset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parental-package",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 21:25:00,204\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.83',\n",
       " 'raylet_ip_address': '192.168.1.83',\n",
       " 'redis_address': '192.168.1.83:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-12-13_21-24-57_791745_81067/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-12-13_21-24-57_791745_81067/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-12-13_21-24-57_791745_81067',\n",
       " 'metrics_export_port': 54449,\n",
       " 'node_id': '40cd6feacbbf61231c9c4366df088c705c5cd6178cf24ea32f876554'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11fcf3",
   "metadata": {},
   "source": [
    "### Define stability target\n",
    "\n",
    "Below, we create a `Vset` which applies three custom data preprocessing functions and another that calculates the permutation importance metric via the function `sklearn.inspection.permutation_importance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complimentary-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vflow import Vset\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "preproc_list = [SimpleImputer(strategy='mean'),\n",
    "                SimpleImputer(strategy='median'),\n",
    "                KNNImputer()]\n",
    "\n",
    "# create a Vset which varies over the list of preprocessing methods\n",
    "preproc_set = Vset(\"preproc\", preproc_list, ['mean', 'med', 'knn'], output_matching=True, lazy=True)\n",
    "\n",
    "# create the feature importance Vset\n",
    "feat_imp_set = build_vset('feat_imp', permutation_importance, n_repeats=3, is_async=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b17b11",
   "metadata": {},
   "source": [
    "### Define model hyperparameter perturbations\n",
    "\n",
    "We can also specify modeling perturbations, both within a single class of models (hyperparameter perturbations) and across different classes. Here we'll use the helper `build_vset` to create hyperparameter perturbations for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spare-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "from vflow import build_vset\n",
    "\n",
    "# hyperparameters to try\n",
    "RF_params = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'min_samples_split': [2, 10]\n",
    "}\n",
    "\n",
    "# we could instead pass a list of distinct models and corresponding param dicts\n",
    "RF_set = build_vset('RF', RF, RF_params, is_async=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d44cfb",
   "metadata": {},
   "source": [
    "### Define data perturbations\n",
    "\n",
    "For stability analysis, it is often useful to add data perturbations such as the bootstrap in order to assess stability over resampling variability in the data. We can lazily compute the bootstrap, such that data will not be resampled until needed, by setting `lazy=True` when constructing a `Vset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finnish-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# create a Vset for bootstrapping from data 10 times\n",
    "# we use lazy=True so that the data will not be resampled until needed\n",
    "boot_set = build_vset('boot', resample, reps=10, lazy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55dd36e",
   "metadata": {},
   "source": [
    "### Fit all models for all combinations of resampling and preprocessing\n",
    "\n",
    "Now we can load in our data and fit each of the four random forest models to the 300 combinations of resampled training data and preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=3, n_informative=1)\n",
    "\n",
    "# 20% of X entries missing\n",
    "i = np.random.randint(X.shape[0], size=round(X.shape[0]*X.shape[1] * 0.2))\n",
    "j = np.random.randint(X.shape[1], size=i.size)\n",
    "X[i, j] = np.nan\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval)\n",
    "\n",
    "X_train, y_train = init_args([X_train, y_train], names=['X_train', 'y_train'])\n",
    "X_val, y_val = init_args([X_val, y_val], names=['X_val', 'y_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "peripheral-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap from training data by calling boot_fun\n",
    "X_trains, y_trains = boot_set(X_train, y_train)\n",
    "\n",
    "# apply three preprocessing methods to each bootstrap sample\n",
    "X_trains = preproc_set.fit_transform(X_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continent-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 389 ms, total: 1.89 s\n",
      "Wall time: 14.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vflow.vset.Vset at 0x7f42986b6430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this results in fitting all 4 RF models to each of the 30 boot/preproc combos\n",
    "RF_set.fit(X_trains, y_trains)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7faf2c",
   "metadata": {},
   "source": [
    "We can examine the pipeline graph to see what happened so far using the utility function `build_graph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "governmental-flooring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f42b9aa2b80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vflow import build_graph\n",
    "\n",
    "# examine the pipeline graph\n",
    "build_graph(RF_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d468ec",
   "metadata": {},
   "source": [
    "### Calculate feature importances and perturbation statistics\n",
    "\n",
    "Finally, we calculate the importance metric and examine its mean and standard deviation across bootstrap perturbations for each combination of data preprocessing and modeling hyperparameters. This allows us to assess the stability of the feature importances conditioned on different pipeline paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "proprietary-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.15 s, sys: 230 ms, total: 2.38 s\n",
      "Wall time: 7.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from vflow import dict_to_df, perturbation_stats\n",
    "\n",
    "# calculate importances\n",
    "importances = feat_imp_set.evaluate(RF_set.out, preproc_set.fit_transform(X_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d9a9e",
   "metadata": {},
   "source": [
    "Multiple outputs can be split with `dict_to_df` using `param_key='out'`. We use it below to split feature importances into mean and std:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "upset-south",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>init-boot</th>\n",
       "      <th>boot</th>\n",
       "      <th>init-preproc</th>\n",
       "      <th>preproc</th>\n",
       "      <th>init-RF</th>\n",
       "      <th>RF</th>\n",
       "      <th>init-feat_imp</th>\n",
       "      <th>init-feat_imp</th>\n",
       "      <th>init-feat_imp</th>\n",
       "      <th>feat_imp</th>\n",
       "      <th>out</th>\n",
       "      <th>out-importances_mean</th>\n",
       "      <th>out-importances_std</th>\n",
       "      <th>out-importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=0,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>mean</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=100, min_samples_split=2)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [0.008761979340984624, -0...</td>\n",
       "      <td>[0.008761979340984624, -0.03558942692257405, 1...</td>\n",
       "      <td>[0.01326207049286781, 0.0034690864117961607, 0...</td>\n",
       "      <td>[[0.02275840447105626, -0.009048209607692481, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=1,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>mean</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=100, min_samples_split=2)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [-0.010073280418657293, 0...</td>\n",
       "      <td>[-0.010073280418657293, 0.012168807698989018, ...</td>\n",
       "      <td>[0.0033459757009757043, 0.007542337590601749, ...</td>\n",
       "      <td>[[-0.008080725092555752, -0.007352620828861567...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=2,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>mean</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=100, min_samples_split=2)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [-0.011914204294315075, 0...</td>\n",
       "      <td>[-0.011914204294315075, 0.009041604244378535, ...</td>\n",
       "      <td>[0.01564443940914106, 0.015410467344513413, 0....</td>\n",
       "      <td>[[-0.012603328568590255, -0.03072079244744419,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=3,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>mean</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=100, min_samples_split=2)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [-0.005313319867751372, 0...</td>\n",
       "      <td>[-0.005313319867751372, 0.009082512805862586, ...</td>\n",
       "      <td>[0.008018689652769454, 0.013622028130875485, 0...</td>\n",
       "      <td>[[-0.0032052164087308377, 0.003282291633944778...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=6,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>knn</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=300, min_samples_split=10)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [0.0016734301804808653, 0...</td>\n",
       "      <td>[0.0016734301804808653, 0.002656624691471617, ...</td>\n",
       "      <td>[0.013277010734487383, 0.008198287925124313, 0...</td>\n",
       "      <td>[[0.0028691181886598693, 0.017303533326473608,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=7,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>knn</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=300, min_samples_split=10)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [0.024444981879042354, -0...</td>\n",
       "      <td>[0.024444981879042354, -0.0006362401761585978,...</td>\n",
       "      <td>[0.010987395218453665, 0.003433490221814577, 0...</td>\n",
       "      <td>[[0.03998322887113548, 0.016595603543394954, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=8,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>knn</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=300, min_samples_split=10)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [0.005365284912533112, -0...</td>\n",
       "      <td>[0.005365284912533112, -0.0565728363177049, 1....</td>\n",
       "      <td>[0.0010804277343730412, 0.021227197736827744, ...</td>\n",
       "      <td>[[0.003913305163604774, 0.005679204113046565, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>X_train</td>\n",
       "      <td>(rep=9,)</td>\n",
       "      <td>X_train</td>\n",
       "      <td>knn</td>\n",
       "      <td>y_train</td>\n",
       "      <td>(n_estimators=300, min_samples_split=10)</td>\n",
       "      <td>X_val</td>\n",
       "      <td>X_val</td>\n",
       "      <td>y_val</td>\n",
       "      <td>feat_imp_0</td>\n",
       "      <td>{'importances_mean': [0.01248406187288457, -0....</td>\n",
       "      <td>[0.01248406187288457, -0.010193188995873848, 1...</td>\n",
       "      <td>[0.008427270679040564, 0.007342116321500198, 0...</td>\n",
       "      <td>[[0.01947683290253177, 0.000629793752883967, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    init-boot      boot init-preproc preproc  init-RF  \\\n",
       "0     X_train  (rep=0,)      X_train    mean  y_train   \n",
       "1     X_train  (rep=1,)      X_train    mean  y_train   \n",
       "2     X_train  (rep=2,)      X_train    mean  y_train   \n",
       "3     X_train  (rep=3,)      X_train    mean  y_train   \n",
       "..        ...       ...          ...     ...      ...   \n",
       "116   X_train  (rep=6,)      X_train     knn  y_train   \n",
       "117   X_train  (rep=7,)      X_train     knn  y_train   \n",
       "118   X_train  (rep=8,)      X_train     knn  y_train   \n",
       "119   X_train  (rep=9,)      X_train     knn  y_train   \n",
       "\n",
       "                                           RF init-feat_imp init-feat_imp  \\\n",
       "0     (n_estimators=100, min_samples_split=2)         X_val         X_val   \n",
       "1     (n_estimators=100, min_samples_split=2)         X_val         X_val   \n",
       "2     (n_estimators=100, min_samples_split=2)         X_val         X_val   \n",
       "3     (n_estimators=100, min_samples_split=2)         X_val         X_val   \n",
       "..                                        ...           ...           ...   \n",
       "116  (n_estimators=300, min_samples_split=10)         X_val         X_val   \n",
       "117  (n_estimators=300, min_samples_split=10)         X_val         X_val   \n",
       "118  (n_estimators=300, min_samples_split=10)         X_val         X_val   \n",
       "119  (n_estimators=300, min_samples_split=10)         X_val         X_val   \n",
       "\n",
       "    init-feat_imp    feat_imp  \\\n",
       "0           y_val  feat_imp_0   \n",
       "1           y_val  feat_imp_0   \n",
       "2           y_val  feat_imp_0   \n",
       "3           y_val  feat_imp_0   \n",
       "..            ...         ...   \n",
       "116         y_val  feat_imp_0   \n",
       "117         y_val  feat_imp_0   \n",
       "118         y_val  feat_imp_0   \n",
       "119         y_val  feat_imp_0   \n",
       "\n",
       "                                                   out  \\\n",
       "0    {'importances_mean': [0.008761979340984624, -0...   \n",
       "1    {'importances_mean': [-0.010073280418657293, 0...   \n",
       "2    {'importances_mean': [-0.011914204294315075, 0...   \n",
       "3    {'importances_mean': [-0.005313319867751372, 0...   \n",
       "..                                                 ...   \n",
       "116  {'importances_mean': [0.0016734301804808653, 0...   \n",
       "117  {'importances_mean': [0.024444981879042354, -0...   \n",
       "118  {'importances_mean': [0.005365284912533112, -0...   \n",
       "119  {'importances_mean': [0.01248406187288457, -0....   \n",
       "\n",
       "                                  out-importances_mean  \\\n",
       "0    [0.008761979340984624, -0.03558942692257405, 1...   \n",
       "1    [-0.010073280418657293, 0.012168807698989018, ...   \n",
       "2    [-0.011914204294315075, 0.009041604244378535, ...   \n",
       "3    [-0.005313319867751372, 0.009082512805862586, ...   \n",
       "..                                                 ...   \n",
       "116  [0.0016734301804808653, 0.002656624691471617, ...   \n",
       "117  [0.024444981879042354, -0.0006362401761585978,...   \n",
       "118  [0.005365284912533112, -0.0565728363177049, 1....   \n",
       "119  [0.01248406187288457, -0.010193188995873848, 1...   \n",
       "\n",
       "                                   out-importances_std  \\\n",
       "0    [0.01326207049286781, 0.0034690864117961607, 0...   \n",
       "1    [0.0033459757009757043, 0.007542337590601749, ...   \n",
       "2    [0.01564443940914106, 0.015410467344513413, 0....   \n",
       "3    [0.008018689652769454, 0.013622028130875485, 0...   \n",
       "..                                                 ...   \n",
       "116  [0.013277010734487383, 0.008198287925124313, 0...   \n",
       "117  [0.010987395218453665, 0.003433490221814577, 0...   \n",
       "118  [0.0010804277343730412, 0.021227197736827744, ...   \n",
       "119  [0.008427270679040564, 0.007342116321500198, 0...   \n",
       "\n",
       "                                       out-importances  \n",
       "0    [[0.02275840447105626, -0.009048209607692481, ...  \n",
       "1    [[-0.008080725092555752, -0.007352620828861567...  \n",
       "2    [[-0.012603328568590255, -0.03072079244744419,...  \n",
       "3    [[-0.0032052164087308377, 0.003282291633944778...  \n",
       "..                                                 ...  \n",
       "116  [[0.0028691181886598693, 0.017303533326473608,...  \n",
       "117  [[0.03998322887113548, 0.016595603543394954, 0...  \n",
       "118  [[0.003913305163604774, 0.005679204113046565, ...  \n",
       "119  [[0.01947683290253177, 0.000629793752883967, 0...  \n",
       "\n",
       "[120 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the helper dict_to_df converts the output to a pandas.DataFrame\n",
    "# using param_key='out' separates the importance dict into multiple cols\n",
    "importances_df = dict_to_df(importances, param_key='out')\n",
    "importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba076236",
   "metadata": {},
   "source": [
    "We can compute statistics on a single iterable item of the output by passing `wrt=out-col` and `split=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "duplicate-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preproc</th>\n",
       "      <th>RF</th>\n",
       "      <th>X-count</th>\n",
       "      <th>X0-mean</th>\n",
       "      <th>X0-std</th>\n",
       "      <th>X1-mean</th>\n",
       "      <th>X1-std</th>\n",
       "      <th>X2-mean</th>\n",
       "      <th>X2-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knn</td>\n",
       "      <td>(n_estimators=100, min_samples_split=10)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>-0.019426</td>\n",
       "      <td>0.020168</td>\n",
       "      <td>1.345494</td>\n",
       "      <td>0.080863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>(n_estimators=100, min_samples_split=2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.012256</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>-0.022497</td>\n",
       "      <td>0.028343</td>\n",
       "      <td>1.371031</td>\n",
       "      <td>0.080874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knn</td>\n",
       "      <td>(n_estimators=300, min_samples_split=10)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>-0.015113</td>\n",
       "      <td>0.025376</td>\n",
       "      <td>1.428892</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>knn</td>\n",
       "      <td>(n_estimators=300, min_samples_split=2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>-0.021748</td>\n",
       "      <td>0.028230</td>\n",
       "      <td>1.385072</td>\n",
       "      <td>0.126618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>med</td>\n",
       "      <td>(n_estimators=100, min_samples_split=10)</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.007374</td>\n",
       "      <td>0.010194</td>\n",
       "      <td>-0.004911</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>1.482730</td>\n",
       "      <td>0.063334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>med</td>\n",
       "      <td>(n_estimators=100, min_samples_split=2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>-0.007913</td>\n",
       "      <td>0.021449</td>\n",
       "      <td>1.503880</td>\n",
       "      <td>0.068309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>med</td>\n",
       "      <td>(n_estimators=300, min_samples_split=10)</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>-0.008601</td>\n",
       "      <td>0.020682</td>\n",
       "      <td>1.465643</td>\n",
       "      <td>0.046286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>med</td>\n",
       "      <td>(n_estimators=300, min_samples_split=2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>0.010353</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.024509</td>\n",
       "      <td>1.496973</td>\n",
       "      <td>0.092036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   preproc                                        RF  X-count   X0-mean  \\\n",
       "0      knn  (n_estimators=100, min_samples_split=10)       10  0.002658   \n",
       "1      knn   (n_estimators=100, min_samples_split=2)       10  0.012256   \n",
       "2      knn  (n_estimators=300, min_samples_split=10)       10  0.007747   \n",
       "3      knn   (n_estimators=300, min_samples_split=2)       10  0.006885   \n",
       "..     ...                                       ...      ...       ...   \n",
       "8      med  (n_estimators=100, min_samples_split=10)       10 -0.007374   \n",
       "9      med   (n_estimators=100, min_samples_split=2)       10  0.008385   \n",
       "10     med  (n_estimators=300, min_samples_split=10)       10 -0.001915   \n",
       "11     med   (n_estimators=300, min_samples_split=2)       10  0.005687   \n",
       "\n",
       "      X0-std   X1-mean    X1-std   X2-mean    X2-std  \n",
       "0   0.007482 -0.019426  0.020168  1.345494  0.080863  \n",
       "1   0.015797 -0.022497  0.028343  1.371031  0.080874  \n",
       "2   0.010101 -0.015113  0.025376  1.428892  0.069200  \n",
       "3   0.010774 -0.021748  0.028230  1.385072  0.126618  \n",
       "..       ...       ...       ...       ...       ...  \n",
       "8   0.010194 -0.004911  0.015810  1.482730  0.063334  \n",
       "9   0.015032 -0.007913  0.021449  1.503880  0.068309  \n",
       "10  0.013333 -0.008601  0.020682  1.465643  0.046286  \n",
       "11  0.010353  0.002442  0.024509  1.496973  0.092036  \n",
       "\n",
       "[12 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get count, mean, and std of importances\n",
    "perturbation_stats(importances_df, 'preproc', 'RF', wrt='out-importances_mean', prefix='X', split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c70f7",
   "metadata": {},
   "source": [
    "From here, we can (optionally) filter over the data preprocessing and modeling perturbations via the helper `filter_vset_by_metric` to select the top combinations in terms of stability (or another metric of interest) and continue our analysis on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "serious-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
